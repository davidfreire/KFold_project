{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting lecture: https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "from DataGenerator import ImgListDataGen\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, it is important to understand differences between ShuffleSplit and KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold\n",
      "TRAIN: [0 2 3 4 5 7 8 9] TEST: [1 6]\n",
      "TRAIN: [0 1 2 3 5 6 7 8] TEST: [4 9]\n",
      "TRAIN: [0 1 3 4 5 6 8 9] TEST: [2 7]\n",
      "TRAIN: [1 2 3 4 6 7 8 9] TEST: [0 5]\n",
      "TRAIN: [0 1 2 4 5 6 7 9] TEST: [3 8]\n",
      "Shuffle Split\n",
      "TRAIN: [8 4 1 0 6 5 7 2] TEST: [3 9]\n",
      "TRAIN: [7 0 3 9 4 5 1 6] TEST: [8 2]\n",
      "TRAIN: [1 2 5 6 4 8 9 0] TEST: [3 7]\n",
      "TRAIN: [4 6 7 8 3 5 1 2] TEST: [9 0]\n",
      "TRAIN: [7 2 6 5 4 3 0 9] TEST: [1 8]\n"
     ]
    }
   ],
   "source": [
    "splits = 5\n",
    "\n",
    "tx = range(10)\n",
    "ty = [0] * 5 + [1] * 5\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import datasets\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "shufflesplit = StratifiedShuffleSplit(n_splits=splits, random_state=42, test_size=2)\n",
    "\n",
    "print(\"KFold\")\n",
    "for train_index, test_index in kfold.split(tx, ty):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "print(\"Shuffle Split\")\n",
    "for train_index, test_index in shufflesplit.split(tx, ty):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In KFolds, each test set should not overlap, even with shuffle. \n",
    "# With KFolds and shuffle, the data is shuffled once at the start, and then divided into the number of desired splits. \n",
    "# The test data is always one of the splits, the train data is the rest.\n",
    "\n",
    "# In ShuffleSplit, the data is shuffled every time, and then split. \n",
    "# This means the test sets may overlap between the splits:\n",
    "#                                Test, first row 3 and third row 3, first row 9 and fourth row 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus, in ShuffleSplit test_size can be specified, for instance .2 means 1-.2 = .8 for training.\n",
    "# While, KFolds depends on the size of the data and K (test set should not overlap), thus, if data is len 10, \n",
    "# and K is 5, then test size is 10/5=2 in order to never overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, let's create the K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_Path = '/Users/dfreire/Dropbox/Datasets/small_dataset/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_KFold(DB_path, k=2):\n",
    "    #Numeric labels\n",
    "    lab = preprocessing.LabelEncoder()\n",
    "    lab.fit(os.listdir(DB_path))\n",
    "    labels = lab.transform(os.listdir(DB_path))\n",
    "    inv = lab.inverse_transform(labels)\n",
    "    class_dict = dict(zip(inv, labels))\n",
    "    print(class_dict)\n",
    "    #inv_class_dict = {k: v for v,k in class_dict.items()} to decode labels if necessary\n",
    "\n",
    "    #Read each folder\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    for class_ in os.listdir(DB_path):\n",
    "        dat = [os.path.join(DB_path, class_, img) for img in os.listdir(os.path.join(DB_path, class_))]\n",
    "        lab = np.ones(len(dat))*class_dict[class_]\n",
    "        \n",
    "        labels = np.concatenate((labels,lab))\n",
    "        data = data + dat\n",
    "    \n",
    "    print(len(data))\n",
    "    print(len(labels))\n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(data, labels))\n",
    "    \n",
    "    return folds, np.array(data), labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "folds, data, labels = Load_KFold(DB_path=DB_Path, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 Folds\n",
      "Train data contains 1500 samples\n",
      "Test data contains 500 samples\n",
      "Test data samples are aprox obtained from 500\n"
     ]
    }
   ],
   "source": [
    "print('There are {} Folds'.format(len(folds)))\n",
    "print('Train data contains {} samples'.format(len(folds[0][0])))\n",
    "print('Test data contains {} samples'.format(len(folds[0][1])))\n",
    "print('Test data samples are aprox obtained from {}'.format(math.ceil(len(data)/k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 250, 1: 250}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the test set of the first fold\n",
    "unique, counts = np.unique(labels[folds[0][1]], return_counts=True)\n",
    "dict(zip(unique, counts))\n",
    "#250 items of class 0 and 250 of class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "['/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.0.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.1.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.101.jpg']\n",
      "\n",
      "Fold  1\n",
      "['/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.1.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.10.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.100.jpg']\n",
      "\n",
      "Fold  2\n",
      "['/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.0.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.10.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.100.jpg']\n",
      "\n",
      "Fold  3\n",
      "['/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.0.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.1.jpg'\n",
      " '/Users/dfreire/Dropbox/Datasets/small_dataset/train/cats/cat.10.jpg']\n"
     ]
    }
   ],
   "source": [
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    \n",
    "    print('\\nFold ',j)\n",
    "    \n",
    "    X_train_cv = data[train_idx]\n",
    "    y_train_cv = labels[train_idx]\n",
    "    X_valid_cv = data[val_idx]\n",
    "    y_valid_cv= labels[val_idx]\n",
    "    \n",
    "    print(X_train_cv[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure is as follows:\n",
    "\n",
    "Shuffle the dataset randomly. ok\n",
    "Split the dataset into k groups ok\n",
    "For each unique group:\n",
    "Take the group as a hold out or test data set\n",
    "Take the remaining groups as a training data set\n",
    "Fit a model on the training set and evaluate it on the test set\n",
    "Retain the evaluation score and discard the model\n",
    "Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, optimizers\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    entrada= Input(shape=(150,150,3))\n",
    "    \n",
    "    conv = Conv2D(filters=32, kernel_size=3, activation='relu', name='conv_1')(entrada)\n",
    "    maxpool = MaxPool2D(pool_size=2, strides=2, name='maxpool_1')(conv)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=3, activation='relu', name='conv_2')(maxpool)\n",
    "    maxpool = MaxPool2D(pool_size=2, strides=2, name='maxpool_2')(conv)   \n",
    "    \n",
    "    conv = Conv2D(filters=128, kernel_size=3, activation='relu', name='conv_3')(maxpool)\n",
    "    maxpool = MaxPool2D(pool_size=2, strides=2, name='maxpool_3')(conv)\n",
    "        \n",
    "    conv = Conv2D(filters=128, kernel_size=3, activation='relu', name='conv_4')(maxpool)\n",
    "    maxpool = MaxPool2D(pool_size=2, strides=2, name='maxpool_4')(conv)\n",
    "    \n",
    "    flat = Flatten(name='flatten')(maxpool)\n",
    "    drop = Dropout(rate=.5, name='dropout')(flat)\n",
    "    \n",
    "    dense = Dense(units=512, activation='relu', name='Dense1')(drop)\n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(dense)\n",
    "    #output = Dense(units=2, activation='softmax', name='output')(dense)\n",
    "    \n",
    "    model = Model(entrada, output)\n",
    "    \n",
    "    #model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "# Parameters\n",
    "train_params = {\n",
    "    'rescale': 1./255,\n",
    "    'batch_size': 100,\n",
    "    'n_classes': 2, \n",
    "    'target_size': (150,150,3),\n",
    "    'aug_mode': 'ShiftScaleRotate',\n",
    "    'class_mode': 'binary' #'categorical',\n",
    "    'shuffle': True}\n",
    "val_params = {\n",
    "    'rescale': 1,#1./255,\n",
    "    'batch_size': 100,\n",
    "    'n_classes': 2, \n",
    "    'target_size': (150,150,3),\n",
    "    'aug_mode': None,\n",
    "    'class_mode': 'binary' #'categorical',\n",
    "    'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    \n",
    "    print('\\nFold ',j)\n",
    "    X_train_cv = data[train_idx]\n",
    "    y_train_cv = labels[train_idx]\n",
    "    X_valid_cv = data[val_idx]\n",
    "    y_valid_cv= labels[val_idx]\n",
    "    \n",
    "    #name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
    "    #callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
    "    training_gen = ImgListDataGen(img_files = X_train_cv, labels=y_train_cv, **train_params)\n",
    "    validation_gen = ImgListDataGen(img_files = X_valid_cv, labels=y_valid_cv, **val_params) \n",
    "    print('Training')                                    \n",
    "    model = get_model()\n",
    "    model.fit_generator(\n",
    "                training_gen,\n",
    "                steps_per_epoch=len(X_train_cv)/batch_size,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data = validation_gen)#,\n",
    "                #callbacks = callbacks)\n",
    "    \n",
    "    print(model.evaluate(X_valid_cv, y_valid_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse from to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 4 4 2 5 3 1 2 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[2 1 4 4 2 5 3 1 2 7]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "from numpy import argmax\n",
    "from keras.utils.np_utils import to_categorical\n",
    "k = 8\n",
    "n = 10\n",
    "x = randint(0, k, (n,))\n",
    "print(x)\n",
    "cat = to_categorical(x, k)\n",
    "print(cat)\n",
    "inv = np.array([val for val in [argmax(i) for i in cat]]) ###\n",
    "print(inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small generator example\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class CIFAR10Sequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        k=np.array([resize(imread(file_name), (150, 150, 3)) for file_name in batch_x])\n",
    "        #print(k.shape)\n",
    "        #print(batch_y)\n",
    "\n",
    "        return np.array([resize(imread(file_name), (150, 150, 3)) for file_name in batch_x]), np.array(batch_y)\n",
    "    \n",
    "    \n",
    "    \n",
    "Xtrain_gen = CIFAR10Sequence(data,labels,batch_size=10) # you can choose your batch size.\n",
    "#Xvalidation_gen = detracSequence(X_validation,y_validation,batch_size=512)\n",
    "\n",
    "model.fit_generator(generator=Xtrain_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
